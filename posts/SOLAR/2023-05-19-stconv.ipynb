{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d58bea-9f8b-421c-a876-cd4a255452ff",
   "metadata": {},
   "source": [
    "ref: <https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/root.html#id18>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc443bc7-d422-4690-928e-d8f0b1280e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv\n",
    "\n",
    "\n",
    "class TemporalConv(nn.Module):\n",
    "    r\"\"\"Temporal convolution block applied to nodes in the STGCN Layer\n",
    "    For details see: `\"Spatio-Temporal Graph Convolutional Networks:\n",
    "    A Deep Learning Framework for Traffic Forecasting.\"\n",
    "    <https://arxiv.org/abs/1709.04875>`_ Based off the temporal convolution\n",
    "     introduced in \"Convolutional Sequence to Sequence Learning\"  <https://arxiv.org/abs/1709.04875>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Convolutional kernel size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3):\n",
    "        super(TemporalConv, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv_2 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "        self.conv_3 = nn.Conv2d(in_channels, out_channels, (1, kernel_size))\n",
    "\n",
    "    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Forward pass through temporal convolution block.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (torch.FloatTensor) -  Input data of shape\n",
    "                (batch_size, input_time_steps, num_nodes, in_channels).\n",
    "\n",
    "        Return types:\n",
    "            * **H** (torch.FloatTensor) - Output data of shape\n",
    "                (batch_size, in_channels, num_nodes, input_time_steps).\n",
    "        \"\"\"\n",
    "        X = X.permute(0, 3, 2, 1)\n",
    "        P = self.conv_1(X)\n",
    "        Q = torch.sigmoid(self.conv_2(X))\n",
    "        PQ = P * Q\n",
    "        H = F.relu(PQ + self.conv_3(X))\n",
    "        H = H.permute(0, 3, 2, 1)\n",
    "        return H\n",
    "\n",
    "\n",
    "\n",
    "class STConv(nn.Module):\n",
    "    r\"\"\"Spatio-temporal convolution block using ChebConv Graph Convolutions.\n",
    "    For details see: `\"Spatio-Temporal Graph Convolutional Networks:\n",
    "    A Deep Learning Framework for Traffic Forecasting\"\n",
    "    <https://arxiv.org/abs/1709.04875>`_\n",
    "\n",
    "    NB. The ST-Conv block contains two temporal convolutions (TemporalConv)\n",
    "    with kernel size k. Hence for an input sequence of length m,\n",
    "    the output sequence will be length m-2(k-1).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        hidden_channels (int): Number of hidden units output by graph convolution block\n",
    "        out_channels (int): Number of output features.\n",
    "        kernel_size (int): Size of the kernel considered.\n",
    "        K (int): Chebyshev filter size :math:`K`.\n",
    "        normalization (str, optional): The normalization scheme for the graph\n",
    "            Laplacian (default: :obj:`\"sym\"`):\n",
    "\n",
    "            1. :obj:`None`: No normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
    "\n",
    "            2. :obj:`\"sym\"`: Symmetric normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
    "            \\mathbf{D}^{-1/2}`\n",
    "\n",
    "            3. :obj:`\"rw\"`: Random-walk normalization\n",
    "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
    "\n",
    "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
    "            this operator in case the normalization is non-symmetric.\n",
    "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
    "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
    "            scalar/zero-dimensional tensor when operating on single graphs.\n",
    "            You can pre-compute :obj:`lambda_max` via the\n",
    "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        K: int,\n",
    "        normalization: str = \"sym\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super(STConv, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.K = K\n",
    "        self.normalization = normalization\n",
    "        self.bias = bias\n",
    "\n",
    "        self._temporal_conv1 = TemporalConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "        )\n",
    "\n",
    "        self._graph_conv = ChebConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            K=K,\n",
    "            normalization=normalization,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "        self._temporal_conv2 = TemporalConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "        )\n",
    "\n",
    "        self._batch_norm = nn.BatchNorm2d(num_nodes)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "\n",
    "        r\"\"\"Forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (PyTorch FloatTensor) - Sequence of node features of shape (Batch size X Input time steps X Num nodes X In channels).\n",
    "            * **edge_index** (PyTorch LongTensor) - Graph edge indices.\n",
    "            * **edge_weight** (PyTorch LongTensor, optional)- Edge weight vector.\n",
    "\n",
    "        Return types:\n",
    "            * **T** (PyTorch FloatTensor) - Sequence of node features.\n",
    "        \"\"\"\n",
    "        T_0 = self._temporal_conv1(X)\n",
    "        T = torch.zeros_like(T_0).to(T_0.device)\n",
    "        for b in range(T_0.size(0)):\n",
    "            for t in range(T_0.size(1)):\n",
    "                T[b][t] = self._graph_conv(T_0[b][t], edge_index, edge_weight)\n",
    "\n",
    "        T = F.relu(T)\n",
    "        T = self._temporal_conv2(T)\n",
    "        T = T.permute(0, 2, 1, 3)\n",
    "        T = self._batch_norm(T)\n",
    "        T = T.permute(0, 2, 1, 3)\n",
    "        return T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262dc1a-6541-4bb2-a17d-bdfa88bb984a",
   "metadata": {},
   "source": [
    "## Windmill Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea03e2e-db53-4455-b9d4-69c141aac0f2",
   "metadata": {},
   "source": [
    "ref: <https://pytorch-geometric-temporal.readthedocs.io/en/latest/modules/dataset.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24597085-4796-4f0c-91fa-65b79bad494a",
   "metadata": {},
   "source": [
    "Hourly energy output of windmills from a European country for more than 2 years. Vertices represent 11 windmills and weighted edges describe the strength of relationships. The target variable allows for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17f3f4c1-77fe-42e7-a805-18824c3ca509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적인 모듈\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx # 그래프 시그널 시각화를 위한 모듈\n",
    "from tqdm import tqdm # for문의 진행 상태 확인\n",
    "import time\n",
    "\n",
    "# 파이토치 관련\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# PyG 관련\n",
    "from torch_geometric.data import Data # 그래프 자료를 만들기 위한 클래스\n",
    "\n",
    "\n",
    "# STGCN 관련\n",
    "import torch_geometric_temporal\n",
    "from torch_geometric_temporal.nn.recurrent import GConvGRU\n",
    "from torch_geometric_temporal.signal import temporal_signal_split # STGCN dataset을 train/test set으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6310e6d7-e306-4c31-ba84-4d77992839ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define recurrent GCN architecture\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features, filters):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = GConvGRU(node_features, filters, 2)\n",
    "        self.linear = torch.nn.Linear(filters, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0381f793-320a-48e8-ae1b-848d0885ed57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric_temporal.signal.static_graph_temporal_signal.StaticGraphTemporalSignal"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric_temporal.signal.StaticGraphTemporalSignal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5374bb3-ecae-46f1-9c5a-f0e6fc1230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89481a4d-bdee-416e-811d-9c9f625a459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = torch_geometric_temporal.dataset.WindmillOutputSmallDatasetLoader()\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "dataset = loader.get_dataset(lags=4)\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "140f9e8e-f87c-441e-8c8e-f1ee4d90519d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentGCN(\n",
       "  (recurrent): GConvGRU(\n",
       "    (conv_x_z): ChebConv(4, 32, K=2, normalization=sym)\n",
       "    (conv_h_z): ChebConv(32, 32, K=2, normalization=sym)\n",
       "    (conv_x_r): ChebConv(4, 32, K=2, normalization=sym)\n",
       "    (conv_h_r): ChebConv(32, 32, K=2, normalization=sym)\n",
       "    (conv_x_h): ChebConv(4, 32, K=2, normalization=sym)\n",
       "    (conv_h_h): ChebConv(32, 32, K=2, normalization=sym)\n",
       "  )\n",
       "  (linear): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RecurrentGCN(node_features=4, filters=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "493fa65c-f2c2-48a0-9d5e-856cc5ac98b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.92817234992981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in tqdm(range(50)):\n",
    "    for t, snapshot in enumerate(train_dataset):\n",
    "        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        cost = torch.mean((yt_hat.reshape(-1)-snapshot.y.reshape(-1))**2)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8f1b50a-6f11-403b-95d5-19a214d13dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.3412\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "cost = cost / (time+1)\n",
    "cost = cost.item()\n",
    "print(\"MSE: {:.4f}\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80350560-4919-4744-baaa-6e7d55a33728",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b72cb516-6dc4-4035-8a7b-8e37a9f787ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd13265d-4603-4682-b865-862a9c295b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.36034393310547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(50)):\n",
    "    for t, snapshot in enumerate(train_dataset):\n",
    "        yt_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).to(device)\n",
    "        cost = torch.mean((yt_hat.reshape(-1)-snapshot.y.reshape(-1).to(device))**2).to(device)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7f16f3f-9992-45d0-9c7b-2048c638b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.9903\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    snapshot = snapshot.to(device)\n",
    "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "cost = cost / (time+1)\n",
    "cost = cost.item()\n",
    "print(\"MSE: {:.4f}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce33f3fe-2fb6-42ad-b68c-b0d954ca9341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 19 04:36:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 30%   47C    P2   125W / 420W |    716MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    237126      C   ...da3/envs/torch/bin/python      714MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
