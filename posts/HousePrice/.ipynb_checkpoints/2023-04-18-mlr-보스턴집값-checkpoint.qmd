---
title: MLR prac1
author: JiyunLim
date: "04/18/2023"
---

# Import

```{r}
library(MASS)
library(lmtest)
```

# Data

```{r}
# 보스턴 집값 데이터 이 데이터는 보스턴 근교 지역의 집값 및 다른 정보를 포함한다.
# MASS 패키지를 설치하면 데이터를 로딩할 수 있다.
data(Boston)
head(Boston)
```

# Data Description

> B보스턴 근교 506개 지역에 대한 범죄율 (crim)등 14개의 변수로 구성 • crim : 범죄율 • zn: 25,000평방비트 기준 거지주 비율 • indus: 비소매업종 점유 구역 비율 • chas: 찰스강 인접 여부 (1=인접, 0=비인접) • nox: 일산화질소 농도 (천만개 당) • rm: 거주지의 평균 방 갯수 ***• age: 1940년 이전에 건축된 주택의 비율 • dis: 보스턴 5대 사업지구와의 거리 • rad: 고속도로 진입용이성 정도 • tax: 재산세율 (10,000달러 당) • ptratio: 학생 대 교사 비율 • black: 1000(B − 0.63)2, B: 아프리카계 미국인 비율 • lstat : 저소득층 비율*** \* • medv: 주택가격의 중앙값 (단위:1,000달러 당)

```{r}
pairs(Boston[,which(names(Boston) %in% 
                      c('medv', 'rm', 'lstat'))], 
      pch=16, col='darkorange')
```

```{r}
# pairs(Boston, pch=16, col='darkorange')
cor(Boston[,which(names(Boston) %in% 
                    c('medv', 'rm', 'lstat'))])
```

# 회귀모형 적합

```{r}
fit_Boston<-lm(medv~rm+lstat, data=Boston)
summary(fit_Boston)
```

![](mlr_boston_prac.png){fig-align="center"}

## matrix

$$\bf{y} = \bf{X}\bf{\beta} + \bf{\epsilon} \Rightarrow \hat{\bf{\beta}}=(\bf{X}^\top X)^{-1}\bf{X}^\top \bf{y}$$

```{r}
n = nrow(Boston)
X = cbind(rep(1,n), Boston$rm, Boston$lstat)
y = Boston$medv
```

```{r}
beta_hat = solve(t(X)%*%X) %*% t(X) %*% y # 행렬곱 (%*%)
beta_hat
```

```{r}
coef(fit_Boston)
```

-   `lm`을 이용해서 하나 행렬을 이용해서 하나 동일한 결과를 얻음을 확인할 수 있다.

$$\hat{y} = \bf{X}\hat{\bf{\beta}}$$

```{r}
y_hat = X %*% beta_hat
y_hat[1:5] # 상위 5개의 값 확인.
```

```{r}
fitted(fit_Boston)[1:5]
```

-   $\hat{y}$도 마찬가지로 동일한 결과를 얻음을 확인할 수 있다.

$$SSE = \sum(y_i - \hat{y}_i)^2,\quad RMSE = \sqrt\frac{SSE}{n-p-1} = \hat{\sigma}$$

```{r}
sse <- sum((y - y_hat)^2) ##SSE
sqrt(sse/(n-2-1)) ##RMSE
summary(fit_Boston)$sigma
```

############################################################## 

```{r}
dt <- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]
head(dt)
```

```{r}
fit_Boston<-lm(medv~., data=dt)
fit_Boston<-lm(medv~rm+lstat, data=dt)

summary(fit_Boston)
## hat y = -1.3583 + 5.0948*rm - 0.6424*lstat
```

## 분산분석 : 회귀직선의 유의성 검정

```{r}
anova(fit_Boston) ## Full model
```

::: callout-tip
$$H_0 : \beta_1=\beta_2=0 \text{ vs. } H_1:not \space H_0$$

-   $H_0$ : 귀무가설, null hypothesis, 영가설 모두 동일한 표현이다.
:::

$H_0: y = \beta_0 \cdot 1$ $H_1: y = \beta_0 \cdot 1 + \beta_1 x_1 + \beta_2 x_2$

```{r}
null_model <- lm(medv~1, data=dt)  # H0
fit_Boston <- lm(medv~., data=dt)  # H1

anova(null_model, fit_Boston) ## null 가설 선택? 설명변수 다쓴 모델 선택?
```

-   Model1은 절편만 쓴 것이고, Model2는 설명변수 2개모두 다 쓴 것이다.
-   RSS와 SSR은 다름을 주의하자.
-   $H_0$ 에 가정된 모형을 선택할지 $H_1$ 에 가정된 모형을 선택할지 F통계량 확인.

![](mlr_boston_prac_anova.png){fig-align="center"}

## $\beta$ 의 신뢰구간

::: callout-important
$\beta_i$ 의 $100(1-\alpha)\%$ $\text{CI}$

-   $\hat{\beta}_i \pm t_{\alpha/2}(n-p-1) \cdot \hat{\text{s.e}}(\hat{\beta}_i)$
:::

::: callout-important
$\beta_i$ 의 covariance-variance matrix

$Var(\beta) = (\bf{X}^\top \bf{X})^{-1} \cdot \sigma^2 = \begin{bmatrix} Var(\beta_0) & Cov(\beta_0, \beta_1) & Cov(\beta_0, \beta_2) \\ Cov(\beta_0, \beta_1) & Var(\beta_1) & Cov(\beta_1, \beta_2) \\ Cov(\beta_0, \beta_2) & Cov(\beta_1, \beta_2) & Var(\beta_2) \end{bmatrix}$
:::

```{r}
# variance-covariance matrix
vcov(fit_Boston)  ##var(hat beta) = (X^TX)^-1 \sigma^2
```

```{r}
confint(fit_Boston, level = 0.95)

```

```{r}
coef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]
coef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]
```

-   $n=506, p =2 \to df = 503$

-   `summary(fit_Boston)$coef[,2]` : covariance를 이용한 표준오차. (summary의 2번째 열의 리턴값.)

## 평균반응, 개별 y 추정

$E(Y|x_0)$ , $y=E(Y|x_0) + \epsilon$

```{r}
new_dt <- data.frame(rm=7, lstat=10)
new_dt
```

```{r}
# hat y0 = -1.3583 + 5.0948*7 - 0.6424*10
predict(fit_Boston, newdata = new_dt)
```

```{r}
c(1,7,10)%*%beta_hat # 절편주의!
```

-   $\hat{y}_0 = x_0^\top \beta = \begin{bmatrix} 1 & 7 & 10\end{bmatrix} \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \\ \hat{\beta}_2\end{bmatrix}$
-   $x_0 = \begin{bmatrix} 1 \\ 7 \\ 10\end{bmatrix}$

```{r}
predict(fit_Boston, 
        newdata = new_dt,
        interval = c("confidence"), 
        level = 0.95)  ##평균반응
```

```{r}
predict(fit_Boston, newdata = new_dt, 
        interval = c("prediction"), 
        level = 0.95)  ## 개별 y
```

-   fit값은 동일하나 구간이 넓어진 것을 확인할 수 있다.

# 절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)

$Model: y = \beta_1 x_1 + \beta_2x_2 + \epsilon$

```{r}
fit_Boston0 <- lm(medv ~ 0 + rm + lstat, dt) # 절편이 없는 모형.
summary(fit_Boston0)
summary(fit_Boston)
```

-   $R^2$ 값만 보면 $63\% \to 94\%$ 로 올라갔으니까 절편이 없는 모형 선택? 해야겠다고 생각할 수 있다.

-   $R^2 = \frac{SSR}{SST} = \frac{\sum(\hat{y}_i - \bar{y})^2}{\sum(y_i-\bar{y})^2}$

-   즉, $R^2$ 는 설명변수 없이 ($y$ 의) 평균만 써서 예측하는 것과 설명변수를 2개를 썻을 때 얼마나 달라지는지 비교하는 것이다.

-   절편이 없는 모형에서 $R^2$ 평균이 기준이 아니라 $0$ 으로부터 (원점)으로부터 얼마나 떨어져 있는 가를 뜻한다. ($R^2 = \frac{\sum(\hat{y}_i - 0)^2}{\sum(y_i-0)^2}$)

-   절편이 있느냐 없느냐에 따라 $R^2$ 의 설명이 조금 달라진다.

-   따라서 절편이 있다 vs. 없다. 를 선택할 때 $R^2$를 기준으로 하기는 에매하다. $\to$ MSE 혹은 RMSE($=\hat{\sigma}$) 를 평가척도로 사용하자.

-   따라서 RMSE 값을 비교해보면 두 모델은 별로 큰 차이가 없다. (절편자체가 별로 유의하지 않았음.)

-   이 경우에는 절편을 빼도 될 것 같다.

## 잔차분석

### $\epsilon$ : 선형성, 등분산성, 정규성, 독립성

```{r}
yhat <- fitted(fit_Boston)
res <- resid(fit_Boston)
```

```{r}
plot(res ~ yhat,pch=16, ylab = 'Residual')
abline(h=0, lty=2, col='grey')
```

-   잔차그림만 봤을 때는 판단하기 좀 에매하다.
-   선형성 좀 에매하다.
-   U자패턴이 나오는 이유는 제곱항을 추가하는 것보다는 오차의 독립성 문제..(제곱항을 추가했을 때 성능이 별로 좋아지지 않음.)

### 등분산성

```{r}
## H0 : 등분산  vs.  H1 : 이분산 (Heteroscedasticity)
bptest(fit_Boston)
```

-   등분산이라고 할 수 있다.

## 잔차의 QQ plot

```{r}
par(mfrow=c(1,2))
qqnorm(res, pch=16)
qqline(res, col = 2)

hist(res)
par(mfrow=c(1,1))
```

## Shapiro-Wilk Test

```{r}
## H0 : normal distribution  vs. H1 : not H0
shapiro.test(res)
```

-   이상치제거해보면 괜찮을 것 같다.

## 독립성검정 : DW test

```{r}
dwtest(fit_Boston, alternative = "two.sided")  #H0 : uncorrelated vs H1 : rho != 0
```

# 가설검정: FM vs. RM

```{r}
reduced_model = lm(medv ~ rm+lstat, data = Boston) # 2개의 설명변수 (q=2)
full_model = lm(medv ~ ., data=Boston) # 13개의 설명변수 (q=13)
```

```{r}
summary(full_model)
summary(reduced_model)
```

-   Full model의 RMSE가 훨씬 줄어들었으며, $adj-R^2$ 도 커졌다. (좋아짐.)
-   그렇다면 Full모델을 쓸지 Reduced model을 쓸지 가설검정을 해보자.

### 가설 설정

$H_0: \beta_1 = \dots = \beta_5 = \beta_7 = \dots \beta_{12} = 0 \quad (RM)$

$H_1: \text{not } H_0$

```{r}
anova(reduced_model, full_model)
```

![](mlr_boston_prac_full_reduced.png){fig-align="center"}

### $F = \frac{(SSE_{RM} - SSE_{FM})/r}{SSE_{FM}/(n-p-1)} = \frac{(SSE_{FM} - SSE_{RM})/r}{SSE_{FM}/(n-p-1)}$

```{r}
p <- full_model$rank-1 
q <- reduced_model$rank-1 
SSE_FM <- anova(full_model)$Sum[p+1] #SSE_FM 
SSE_RM <- anova(reduced_model)$Sum[q+1] #SSE_RM

F0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(nrow(Boston)-p-1)) 
F0
```


### 기각역 $F_{0.05}(p-q,n-p-1)$

```{r}
qf(0.95,p-q,nrow(Boston)-p-1) 
# p-value 
1-pf(F0, p-q,nrow(Boston)-p-1)
```

################################# 
$H_0: \beta_{indus} = \beta_{age} = 0$

$H_1: \text{not } H_0$

```{r}
reduced_model = lm(medv ~ .-age-indus, data = Boston) # q=11, r=2
full_model = lm(medv ~ ., data=Boston) # p=13

anova(reduced_model, full_model)
```
![](mlr_boston_prac_f_dist.png){fig-align="center"}

############################################################ 


# General linear hypothesis
> 판매량 광고비 데이터.

```{r}
x1<-c(4,8,9,8,8,12,6,10,6,9) 
x2<-c(4,10,8,5,10,15,8,13,5,12) 
y<-c(9,20,22,15,17,30,18,25,10,20) 
fit<-lm(y~x1+x2) ##FM 
summary(fit)
```

## $H0 : T\beta = c$

### $H_0 : \beta_1 = 1$


즉, $H_0: \beta_1 = 1$ 이라는 것은 다음과 같다.

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$

$H_0: y = \beta_0 + x_1 + \beta_2x_2 + \epsilon \\
\Rightarrow y-x_1 = \beta_0 + \beta_2x_2 + \epsilon \\
\Rightarrow z  = \beta_0 + \beta_2x_2 + \epsilon$

위와 같이 변수변환을 하게 되면 결국 단순선형회귀가 된다.

> 개별회귀계수 유의성 검정인 t검정 해도 됩니다. $\big(t_0 = \frac{\hat{\beta}_1-1}{\hat{\text{s.e}(\hat{\beta}_1)}}\big)$


```{r, message=FALSE}
library(car)
```

```{r}
#H_0 : beta_1 = 1
linearHypothesis(fit, c(0,1,0), 1)
```
- 귀무가설을 기각할 수 없다.


```{r}
#b1-b2=0 => (0,1,-1) *beta 
#H_0 : beta_1 = beta2
linearHypothesis(fit, c(0,1,-1), 0)
```

```{r}
#H_0 : beta_1 = beta2 + 1
linearHypothesis(fit, c(0,1,-1), 1)
```
- 데이터 수가 적기 때문에 통계량 값 자체가 작아질 수 밖에 없다. (웬만하면 기각못함.)
- 표준오차가 작으려면 데이터 수가 많아야한다.


```{r}
#H_0 : beta_1 = beta2 + 5
linearHypothesis(fit, c(0,1,-1), 5)
```
또는 다음의 방법으로 검정을 수행할 수 있다.


```{r}
##H_0 : beta_1 = beta2 + 1
#y=b0 + b1x1 + b2x2 + e = b0+x1 + b2(x1+x2)+e
#y-x1 = b0+b2(x1+x2)+e :   RM
```

```{r}
y1 <- y-x1
z1 <- x1 + x2
```

```{r}
fit2 <- lm(y1~z1)
summary(fit2)
anova(fit2)
```



```{r}
anova(fit)  ##FM
anova(fit2)  #RM
```

```{r}
# F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}
p <- fit$rank-1
q <- fit2$rank-1
SSE_FM <- anova(fit)$Sum[p+1] #SSE_FM
SSE_RM <- anova(fit2)$Sum[q+1]  #SSE_RM

F0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(length(y)-p-1))
F0
```

```{r}
#기각역 F_{0.05}(p-q,n-p-1)
qf(0.95,p-q,length(y)-p-1)
```

```{r}
# p-value
pf(F0, p-q,length(y)-p-1,lower.tail = F)
```
