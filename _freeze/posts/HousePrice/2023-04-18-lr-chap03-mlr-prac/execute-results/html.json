{
  "hash": "17e7f7f41ce09e28e986605bd7424e02",
  "result": {
    "markdown": "---\ntitle: 3. MLR 실습\nauthor: JiyunLim\ndate: \"04/18/2023\"\n---\n\n\n# Import\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(lmtest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: zoo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'zoo'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n:::\n:::\n\n\n# Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 보스턴 집값 데이터 이 데이터는 보스턴 근교 지역의 집값 및 다른 정보를 포함한다.\n# MASS 패키지를 설치하면 데이터를 로딩할 수 있다.\ndata(Boston)\nhead(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n```\n:::\n:::\n\n\n# Data Description\n\n> B보스턴 근교 506개 지역에 대한 범죄율 (crim)등 14개의 변수로 구성 \n\n• crim : 범죄율 \n\n• zn: 25,000평방비트 기준 거지주 비율 \n\n• indus: 비소매업종 점유 구역 비율 \n\n• chas: 찰스강 인접 여부 (1=인접, 0=비인접) \n\n• nox: 일산화질소 농도 (천만개 당) \n\n• rm: 거주지의 평균 방 갯수 \n\n• age: 1940년 이전에 건축된 주택의 비율 \n\n• dis: 보스턴 5대 사업지구와의 거리 \n\n• rad: 고속도로 진입용이성 정도 \n\n• tax: 재산세율 (10,000달러 당) \n\n• ptratio: 학생 대 교사 비율 \n\n• black: 1000(B − 0.63)2, B: 아프리카계 미국인 비율 \n\n• lstat : 저소득층 비율\n\n• medv: 주택가격의 중앙값 (단위:1,000달러 당)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(Boston[,which(names(Boston) %in% \n                      c('medv', 'rm', 'lstat'))], \n      pch=16, col='darkorange')\n```\n\n::: {.cell-output-display}\n![](2023-04-18-lr-chap03-mlr-prac_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# pairs(Boston, pch=16, col='darkorange')\ncor(Boston[,which(names(Boston) %in% \n                    c('medv', 'rm', 'lstat'))])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              rm      lstat       medv\nrm     1.0000000 -0.6138083  0.6953599\nlstat -0.6138083  1.0000000 -0.7376627\nmedv   0.6953599 -0.7376627  1.0000000\n```\n:::\n:::\n\n\n# 회귀모형 적합\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Boston<-lm(medv~rm+lstat, data=Boston)\nsummary(fit_Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,\tAdjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n![](mlr_boston_prac.png){fig-align=\"center\"}\n\n## matrix\n\n$$\\bf{y} = \\bf{X}\\bf{\\beta} + \\bf{\\epsilon} \\Rightarrow \\hat{\\bf{\\beta}}=(\\bf{X}^\\top X)^{-1}\\bf{X}^\\top \\bf{y}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = nrow(Boston)\nX = cbind(rep(1,n), Boston$rm, Boston$lstat)\ny = Boston$medv\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat = solve(t(X)%*%X) %*% t(X) %*% y # 행렬곱 (%*%)\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]\n[1,] -1.3582728\n[2,]  5.0947880\n[3,] -0.6423583\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(fit_Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          rm       lstat \n -1.3582728   5.0947880  -0.6423583 \n```\n:::\n:::\n\n\n-   `lm`을 이용해서 하나 행렬을 이용해서 하나 동일한 결과를 얻음을 확인할 수 있다.\n\n$$\\hat{y} = \\bf{X}\\hat{\\bf{\\beta}}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_hat = X %*% beta_hat\ny_hat[1:5] # 상위 5개의 값 확인.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 28.94101 25.48421 32.65907 32.40652 31.63041\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(fit_Boston)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3        4        5 \n28.94101 25.48421 32.65907 32.40652 31.63041 \n```\n:::\n:::\n\n\n-   $\\hat{y}$도 마찬가지로 동일한 결과를 얻음을 확인할 수 있다.\n\n$$SSE = \\sum(y_i - \\hat{y}_i)^2,\\quad RMSE = \\sqrt\\frac{SSE}{n-p-1} = \\hat{\\sigma}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse <- sum((y - y_hat)^2) ##SSE\nsqrt(sse/(n-2-1)) ##RMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.540257\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Boston)$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.540257\n```\n:::\n:::\n\n\n############################################################## \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt <- Boston[,which(names(Boston) %in% c('medv', 'rm', 'lstat'))]\nhead(dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     rm lstat medv\n1 6.575  4.98 24.0\n2 6.421  9.14 21.6\n3 7.185  4.03 34.7\n4 6.998  2.94 33.4\n5 7.147  5.33 36.2\n6 6.430  5.21 28.7\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Boston<-lm(medv~., data=dt)\nfit_Boston<-lm(medv~rm+lstat, data=dt)\n\nsummary(fit_Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,\tAdjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n## hat y = -1.3583 + 5.0948*rm - 0.6424*lstat\n```\n:::\n\n\n## 분산분석 : 회귀직선의 유의성 검정\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit_Boston) ## Full model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: medv\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nrm          1 20654.4 20654.4  672.90 < 2.2e-16 ***\nlstat       1  6622.6  6622.6  215.76 < 2.2e-16 ***\nResiduals 503 15439.3    30.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n::: callout-tip\n$$H_0 : \\beta_1=\\beta_2=0 \\text{ vs. } H_1:not \\space H_0$$\n\n-   $H_0$ : 귀무가설, null hypothesis, 영가설 모두 동일한 표현이다.\n:::\n\n$H_0: y = \\beta_0 \\cdot 1$ $H_1: y = \\beta_0 \\cdot 1 + \\beta_1 x_1 + \\beta_2 x_2$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_model <- lm(medv~1, data=dt)  # H0\nfit_Boston <- lm(medv~., data=dt)  # H1\n\nanova(null_model, fit_Boston) ## null 가설 선택? 설명변수 다쓴 모델 선택?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: medv ~ 1\nModel 2: medv ~ rm + lstat\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1    505 42716                                  \n2    503 15439  2     27277 444.33 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n-   Model1은 절편만 쓴 것이고, Model2는 설명변수 2개모두 다 쓴 것이다.\n-   RSS와 SSR은 다름을 주의하자.\n-   $H_0$ 에 가정된 모형을 선택할지 $H_1$ 에 가정된 모형을 선택할지 F통계량 확인.\n\n![](mlr_boston_prac_anova.png){fig-align=\"center\"}\n\n## $\\beta$ 의 신뢰구간\n\n::: callout-important\n$\\beta_i$ 의 $100(1-\\alpha)\\%$ $\\text{CI}$\n\n-   $\\hat{\\beta}_i \\pm t_{\\alpha/2}(n-p-1) \\cdot \\hat{\\text{s.e}}(\\hat{\\beta}_i)$\n:::\n\n::: callout-important\n$\\beta_i$ 의 covariance-variance matrix\n\n$Var(\\beta) = (\\bf{X}^\\top \\bf{X})^{-1} \\cdot \\sigma^2 = \\begin{bmatrix} Var(\\beta_0) & Cov(\\beta_0, \\beta_1) & Cov(\\beta_0, \\beta_2) \\\\ Cov(\\beta_0, \\beta_1) & Var(\\beta_1) & Cov(\\beta_1, \\beta_2) \\\\ Cov(\\beta_0, \\beta_2) & Cov(\\beta_1, \\beta_2) & Var(\\beta_2) \\end{bmatrix}$\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# variance-covariance matrix\nvcov(fit_Boston)  ##var(hat beta) = (X^TX)^-1 \\sigma^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          rm        lstat\n(Intercept) 10.06683612 -1.39248641 -0.099178133\nrm          -1.39248641  0.19754958  0.011930670\nlstat       -0.09917813  0.01193067  0.001912441\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(fit_Boston, level = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n(Intercept) -7.5919003  4.8753547\nrm           4.2215504  5.9680255\nlstat       -0.7282772 -0.5564395\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(fit_Boston) + qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          rm       lstat \n  4.8753547   5.9680255  -0.5564395 \n```\n:::\n\n```{.r .cell-code}\ncoef(fit_Boston) - qt(0.975, 503) * summary(fit_Boston)$coef[,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)          rm       lstat \n -7.5919003   4.2215504  -0.7282772 \n```\n:::\n:::\n\n\n-   $n=506, p =2 \\to df = 503$\n\n-   `summary(fit_Boston)$coef[,2]` : covariance를 이용한 표준오차. (summary의 2번째 열의 리턴값.)\n\n## 평균반응, 개별 y 추정\n\n$E(Y|x_0)$ , $y=E(Y|x_0) + \\epsilon$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_dt <- data.frame(rm=7, lstat=10)\nnew_dt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  rm lstat\n1  7    10\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# hat y0 = -1.3583 + 5.0948*7 - 0.6424*10\npredict(fit_Boston, newdata = new_dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n27.88166 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nc(1,7,10)%*%beta_hat # 절편주의!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 27.88166\n```\n:::\n:::\n\n\n-   $\\hat{y}_0 = x_0^\\top \\beta = \\begin{bmatrix} 1 & 7 & 10\\end{bmatrix} \\begin{bmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2\\end{bmatrix}$\n-   $x_0 = \\begin{bmatrix} 1 \\\\ 7 \\\\ 10\\end{bmatrix}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit_Boston, \n        newdata = new_dt,\n        interval = c(\"confidence\"), \n        level = 0.95)  ##평균반응\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 27.88166 27.17347 28.58985\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(fit_Boston, newdata = new_dt, \n        interval = c(\"prediction\"), \n        level = 0.95)  ## 개별 y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 27.88166 16.97375 38.78957\n```\n:::\n:::\n\n\n-   fit값은 동일하나 구간이 넓어진 것을 확인할 수 있다.\n\n# 절편을 포함하지 않는 회귀직선 (원점을 지나는 회귀직선)\n\n$Model: y = \\beta_1 x_1 + \\beta_2x_2 + \\epsilon$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_Boston0 <- lm(medv ~ 0 + rm + lstat, dt) # 절편이 없는 모형.\nsummary(fit_Boston0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ 0 + rm + lstat, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.714  -3.498  -1.075   1.877  27.750 \n\nCoefficients:\n      Estimate Std. Error t value Pr(>|t|)    \nrm     4.90691    0.07019   69.91   <2e-16 ***\nlstat -0.65574    0.03056  -21.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.536 on 504 degrees of freedom\nMultiple R-squared:  0.9485,\tAdjusted R-squared:  0.9482 \nF-statistic:  4637 on 2 and 504 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(fit_Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,\tAdjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   $R^2$ 값만 보면 $63\\% \\to 94\\%$ 로 올라갔으니까 절편이 없는 모형 선택? 해야겠다고 생각할 수 있다.\n\n-   $R^2 = \\frac{SSR}{SST} = \\frac{\\sum(\\hat{y}_i - \\bar{y})^2}{\\sum(y_i-\\bar{y})^2}$\n\n-   즉, $R^2$ 는 설명변수 없이 ($y$ 의) 평균만 써서 예측하는 것과 설명변수를 2개를 썻을 때 얼마나 달라지는지 비교하는 것이다.\n\n-   절편이 없는 모형에서 $R^2$ 평균이 기준이 아니라 $0$ 으로부터 (원점)으로부터 얼마나 떨어져 있는 가를 뜻한다. ($R^2 = \\frac{\\sum(\\hat{y}_i - 0)^2}{\\sum(y_i-0)^2}$)\n\n-   절편이 있느냐 없느냐에 따라 $R^2$ 의 설명이 조금 달라진다.\n\n-   따라서 절편이 있다 vs. 없다. 를 선택할 때 $R^2$를 기준으로 하기는 에매하다. $\\to$ MSE 혹은 RMSE($=\\hat{\\sigma}$) 를 평가척도로 사용하자.\n\n-   따라서 RMSE 값을 비교해보면 두 모델은 별로 큰 차이가 없다. (절편자체가 별로 유의하지 않았음.)\n\n-   이 경우에는 절편을 빼도 될 것 같다.\n\n## 잔차분석\n\n### $\\epsilon$ : 선형성, 등분산성, 정규성, 독립성\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat <- fitted(fit_Boston)\nres <- resid(fit_Boston)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(res ~ yhat,pch=16, ylab = 'Residual')\nabline(h=0, lty=2, col='grey')\n```\n\n::: {.cell-output-display}\n![](2023-04-18-lr-chap03-mlr-prac_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n-   잔차그림만 봤을 때는 판단하기 좀 에매하다.\n-   선형성 좀 에매하다.\n-   U자패턴이 나오는 이유는 제곱항을 추가하는 것보다는 오차의 독립성 문제..(제곱항을 추가했을 때 성능이 별로 좋아지지 않음.)\n\n### 등분산성\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## H0 : 등분산  vs.  H1 : 이분산 (Heteroscedasticity)\nbptest(fit_Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  fit_Boston\nBP = 1.5297, df = 2, p-value = 0.4654\n```\n:::\n:::\n\n\n-   등분산이라고 할 수 있다.\n\n## 잔차의 QQ plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nqqnorm(res, pch=16)\nqqline(res, col = 2)\n\nhist(res)\n```\n\n::: {.cell-output-display}\n![](2023-04-18-lr-chap03-mlr-prac_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\n## Shapiro-Wilk Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## H0 : normal distribution  vs. H1 : not H0\nshapiro.test(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  res\nW = 0.9098, p-value < 2.2e-16\n```\n:::\n:::\n\n\n-   이상치제거해보면 괜찮을 것 같다.\n\n## 독립성검정 : DW test\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndwtest(fit_Boston, alternative = \"two.sided\")  #H0 : uncorrelated vs H1 : rho != 0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tDurbin-Watson test\n\ndata:  fit_Boston\nDW = 0.83421, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n```\n:::\n:::\n\n\n# 가설검정: FM vs. RM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduced_model = lm(medv ~ rm+lstat, data = Boston) # 2개의 설명변수 (q=2)\nfull_model = lm(medv ~ ., data=Boston) # 13개의 설명변수 (q=13)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(full_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  < 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,\tAdjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(reduced_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ rm + lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.076  -3.516  -1.010   1.909  28.131 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.35827    3.17283  -0.428    0.669    \nrm           5.09479    0.44447  11.463   <2e-16 ***\nlstat       -0.64236    0.04373 -14.689   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.54 on 503 degrees of freedom\nMultiple R-squared:  0.6386,\tAdjusted R-squared:  0.6371 \nF-statistic: 444.3 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   Full model의 RMSE가 훨씬 줄어들었으며, $adj-R^2$ 도 커졌다. (좋아짐.)\n-   그렇다면 Full모델을 쓸지 Reduced model을 쓸지 가설검정을 해보자.\n\n### 가설 설정\n\n$H_0: \\beta_1 = \\dots = \\beta_5 = \\beta_7 = \\dots \\beta_{12} = 0 \\quad (RM)$\n\n$H_1: \\text{not } H_0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(reduced_model, full_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: medv ~ rm + lstat\nModel 2: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1    503 15439                                  \n2    492 11079 11    4360.5 17.604 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n![](mlr_boston_prac_full_reduced.png){fig-align=\"center\"}\n\n### $F = \\frac{(SSE_{RM} - SSE_{FM})/r}{SSE_{FM}/(n-p-1)} = \\frac{(SSE_{FM} - SSE_{RM})/r}{SSE_{FM}/(n-p-1)}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- full_model$rank-1 \nq <- reduced_model$rank-1 \nSSE_FM <- anova(full_model)$Sum[p+1] #SSE_FM \nSSE_RM <- anova(reduced_model)$Sum[q+1] #SSE_RM\n\nF0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(nrow(Boston)-p-1)) \nF0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 17.60431\n```\n:::\n:::\n\n\n\n### 기각역 $F_{0.05}(p-q,n-p-1)$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqf(0.95,p-q,nrow(Boston)-p-1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.808117\n```\n:::\n\n```{.r .cell-code}\n# p-value \n1-pf(F0, p-q,nrow(Boston)-p-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n################################# \n$H_0: \\beta_{indus} = \\beta_{age} = 0$\n\n$H_1: \\text{not } H_0$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduced_model = lm(medv ~ .-age-indus, data = Boston) # q=11, r=2\nfull_model = lm(medv ~ ., data=Boston) # p=13\n\nanova(reduced_model, full_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: medv ~ (crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat) - age - indus\nModel 2: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + \n    tax + ptratio + black + lstat\n  Res.Df   RSS Df Sum of Sq      F Pr(>F)\n1    494 11081                           \n2    492 11079  2    2.5794 0.0573 0.9443\n```\n:::\n:::\n\n![](mlr_boston_prac_f_dist.png){fig-align=\"center\"}\n\n############################################################ \n\n\n# General linear hypothesis\n> 판매량 광고비 데이터.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx1<-c(4,8,9,8,8,12,6,10,6,9) \nx2<-c(4,10,8,5,10,15,8,13,5,12) \ny<-c(9,20,22,15,17,30,18,25,10,20) \nfit<-lm(y~x1+x2) ##FM \nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4575 -1.9100  0.3314  0.6388  3.2628 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.6507     2.9075  -0.224   0.8293  \nx1            1.5515     0.6462   2.401   0.0474 *\nx2            0.7599     0.3968   1.915   0.0970 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.278 on 7 degrees of freedom\nMultiple R-squared:  0.9014,\tAdjusted R-squared:  0.8732 \nF-statistic:    32 on 2 and 7 DF,  p-value: 0.0003011\n```\n:::\n:::\n\n\n## $H0 : T\\beta = c$\n\n### $H_0 : \\beta_1 = 1$\n\n\n즉, $H_0: \\beta_1 = 1$ 이라는 것은 다음과 같다.\n\n$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$\n\n$H_0: y = \\beta_0 + x_1 + \\beta_2x_2 + \\epsilon \\\\\n\\Rightarrow y-x_1 = \\beta_0 + \\beta_2x_2 + \\epsilon \\\\\n\\Rightarrow z  = \\beta_0 + \\beta_2x_2 + \\epsilon$\n\n위와 같이 변수변환을 하게 되면 결국 단순선형회귀가 된다.\n\n> 개별회귀계수 유의성 검정인 t검정 해도 됩니다. $\\big(t_0 = \\frac{\\hat{\\beta}_1-1}{\\hat{\\text{s.e}(\\hat{\\beta}_1)}}\\big)$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#H_0 : beta_1 = 1\nlinearHypothesis(fit, c(0,1,0), 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nx1 = 1\n\nModel 1: restricted model\nModel 2: y ~ x1 + x2\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      8 40.105                           \n2      7 36.326  1    3.7788 0.7282 0.4217\n```\n:::\n:::\n\n- 귀무가설을 기각할 수 없다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#b1-b2=0 => (0,1,-1) *beta \n#H_0 : beta_1 = beta2\nlinearHypothesis(fit, c(0,1,-1), 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nx1 - x2 = 0\n\nModel 1: restricted model\nModel 2: y ~ x1 + x2\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      8 39.535                           \n2      7 36.326  1     3.209 0.6184 0.4574\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#H_0 : beta_1 = beta2 + 1\nlinearHypothesis(fit, c(0,1,-1), 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nx1 - x2 = 1\n\nModel 1: restricted model\nModel 2: y ~ x1 + x2\n\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      8 36.549                           \n2      7 36.326  1   0.22253 0.0429 0.8418\n```\n:::\n:::\n\n- 데이터 수가 적기 때문에 통계량 값 자체가 작아질 수 밖에 없다. (웬만하면 기각못함.)\n- 표준오차가 작으려면 데이터 수가 많아야한다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#H_0 : beta_1 = beta2 + 5\nlinearHypothesis(fit, c(0,1,-1), 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear hypothesis test\n\nHypothesis:\nx1 - x2 = 5\n\nModel 1: restricted model\nModel 2: y ~ x1 + x2\n\n  Res.Df     RSS Df Sum of Sq     F   Pr(>F)   \n1      8 127.035                               \n2      7  36.326  1    90.709 17.48 0.004133 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n또는 다음의 방법으로 검정을 수행할 수 있다.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##H_0 : beta_1 = beta2 + 1\n#y=b0 + b1x1 + b2x2 + e = b0+x1 + b2(x1+x2)+e\n#y-x1 = b0+b2(x1+x2)+e :   RM\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ny1 <- y-x1\nz1 <- x1 + x2\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- lm(y1~z1)\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y1 ~ z1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5054 -1.9294  0.4236  0.6821  3.4473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -1.0014     2.2175  -0.452 0.663574    \nz1            0.6824     0.1242   5.493 0.000578 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.137 on 8 degrees of freedom\nMultiple R-squared:  0.7904,\tAdjusted R-squared:  0.7642 \nF-statistic: 30.17 on 1 and 8 DF,  p-value: 0.0005785\n```\n:::\n\n```{.r .cell-code}\nanova(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y1\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nz1         1 137.851 137.851  30.174 0.0005785 ***\nResiduals  8  36.549   4.569                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit)  ##FM\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y\n          Df  Sum Sq Mean Sq F value  Pr(>F)    \nx1         1 313.043 313.043 60.3231 0.00011 ***\nx2         1  19.030  19.030  3.6671 0.09704 .  \nResiduals  7  36.326   5.189                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nanova(fit2)  #RM\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: y1\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nz1         1 137.851 137.851  30.174 0.0005785 ***\nResiduals  8  36.549   4.569                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# F = {(SSE_RM - SSE_FM)/r} / {SSE_FM/(n-p-1)}\np <- fit$rank-1\nq <- fit2$rank-1\nSSE_FM <- anova(fit)$Sum[p+1] #SSE_FM\nSSE_RM <- anova(fit2)$Sum[q+1]  #SSE_RM\n\nF0 <- ((SSE_RM-SSE_FM)/(p-q))/(SSE_FM/(length(y)-p-1))\nF0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.04288074\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#기각역 F_{0.05}(p-q,n-p-1)\nqf(0.95,p-q,length(y)-p-1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.591448\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# p-value\npf(F0, p-q,length(y)-p-1,lower.tail = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.841845\n```\n:::\n:::\n",
    "supporting": [
      "2023-04-18-lr-chap03-mlr-prac_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}